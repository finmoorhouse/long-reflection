---
title: Further reading
description: The about page.
---
This page contains excerpts from writing about the Long Reflection, as well as relevant further reading.

### The Precipice (2020)

The idea of the Long Reflection was first discussed in writing in Toby Ord’s [The Precipice](https://theprecipice.com/) (2020). Ord considers the Long Reflection as one stage in a possible ‘grand strategy for humanity’ — one way to think about helping humanity achieve its potential. The first stage, reaching existential security, is the book’s main concern. The second stage — in priority and order — is the Long Reflection. The third stage is achieving our potential — enacting the results of the Long Reflection.

This excerpt is from Chapter 7, pp. 191–193.

<hr/>

If we achieve existential security, we will have room to breathe. With humanity’s longterm potential secured, we will be past the Precipice, free to contemplate the range of futures that lie open before us. And we will be able to take our time to reflect upon what we truly desire; upon which of these visions for humanity would be the best realization of our potential. We shall call this the Long Reflection.

We rarely think this way. We focus on the here and now. Even those of us who care deeply about the longterm future need to focus most of our attention on making sure we have a future. But once we achieve existential security, we will have the luxury of time in which to compare the kinds of futures available to us and judge which is best. Most work in moral philosophy so far has focused on negatives—on avoiding wrong action and bad outcomes. The study of the positive is at a much earlier stage of development. During the Long Reflection, we would need to develop mature theories that allow us to compare the grand accomplishments our descendants might achieve with eons and galaxies as their canvas.

Present-day humans, myself included, are poorly positioned to anticipate the results of this reflection. But we are uniquely positioned to make it possible.

The ultimate aim of the Long Reflection would be to achieve a final answer to the question of which is the best kind of future for humanity. This may be the true answer (if truth is applicable to moral questions) or failing that, the answer we would converge to under an ideal process of reflection. It may be that even convergence is impossible, with some disputes or uncertainties that are beyond the power of reason to resolve. If so, our aim would be to find the future that gave the best possible conciliation between the remaining perspectives.

We would not need to fully complete this process before moving forward. What is essential is to be sufficiently confident in the broad shape of what we are aiming at before taking each bold and potentially irreversible action—each action that could plausibly lock in substantial aspects of our future trajectory.

For example, it may be that the best achievable future involves physically perfecting humanity, by genetically improving our biology. Or it may involve giving people the freedom to adopt a stunning diversity of new biological forms. But proceeding down either of these paths prematurely could introduce its own existential risks.

If we radically change our nature, we replace humanity (or at least Homo sapiens) with something new. This would risk losing what was most valuable about humanity before truly coming to understand it. If we diversify our forms, we fragment humanity. We might lose the essential unity of humanity that allows a common vision for our future, and instead find ourselves in a perpetual struggle or unsatisfactory compromise. Other bold actions could pose similar risks, for instance spreading out beyond our Solar System into a federation of independent worlds, each drifting in its own cultural direction.

This is not to reject such changes to the human condition—they may well be essential to realizing humanity’s full potential. What I am saying is that these are the kind of bold changes that would need to come after the Long Reflection. Or at least after enough reflection to fully understand the consequences of that particular change. We need to take our time, and choose our path with great care. For once we have existential security we are almost assured success if we take things slowly and carefully: the game is ours to lose; there are only unforced errors.

What can we say about the process of the Long Reflection? I am not imagining this as the sole task of humanity during that time—there would be many other great projects, such as the continuing quests for knowledge, prosperity and justice. And many of the people at the time may have only passing interest in the Long Reflection. But it is the Long Reflection that would have the most bearing on the shape of the future, and so it would be this for which the time would be remembered.

The process may take place largely within intellectual circles, or within the wider public sphere. Either way, we would need to take the greatest care to avoid it being shaped by the bias or prejudice of those involved. As Jonathan Schell said regarding a similar venture, “even if every person in the world were to enlist, the endeavour would include only an infinitesimal fraction of the people of the dead and the unborn generations, and so it would need to act with the circumspection and modesty of a small minority.” While the conversation should be courteous and respectful to all perspectives, it is even more important that it be robust and rigorous. For its ultimate aim is not just to win the goodwill of those alive at the time, but to deliver a verdict that stands the test of eternity.

While moral philosophy would play a central role, the Long Reflection would require insights from many disciplines. For it isn’t just about determining which futures are best, but which are feasible in the first place, and which strategies are most likely to bring them about. This requires analysis from science, engineering, economics, political theory and beyond.

We could think of these first two steps of existential security and the Long Reflection as designing a constitution for humanity. Achieving existential security would be like writing the safeguarding of our potential into our constitution. The Long Reflection would then flesh out this constitution, setting the directions and limits in which our future will unfold.

Our ultimate aim, of course, is the final step: fully achieving humanity’s potential.14 But this can wait upon a serious reflection about which future is best and on how to achieve that future without any fatal missteps. And while it would not hurt to begin such reflection now, it is not the most urgent task. To maximize our chance of success, we need first to get ourselves to safety—to achieve existential security. This is the task of our time. The rest can wait.

### What We Owe The Future (2022)

[What We Owe The Future](https://whatweowethefuture.com/uk/) (2022), by William MacAskill, discusses ways to protect and improve the lives of future generations.

This excerpt is from Chapter 4,, pp. 98–99.

<hr/>

As an ideal, we could aim for what we can call the long reflection: a stable state of the world in which we are safe from calamity and we can reflect on and debate the nature of the good life, working out what the most flourishing society would be. I call this the “long” reflection not because of how long this period would last but because of how long it would be worth spending on it. It’s worth spending five minutes to decide where to spend two hours at dinner; it’s worth spending months to choose a profession for the rest of one’s life. But civilisation might last millions, billions, or even trillions of years. It would therefore be worth spending many centuries to ensure that we’ve really figured things out before we take irreversible actions like locking in values or spreading across the stars.

It seems unlikely to me that anything like the long reflection will occur. But we can see it as an ideal to try to approximate. What we want to do is build a morally exploratory world: one structured so that, over time, the norms and institutions that are morally better are more likely to win out, leading us, over time, to converge on the best possible society. This would involve several things.

First, we would need to keep our options open as much as possible. This gives us a reason, though not necessarily a decisive reason, to delay events which risk value lock-in. Such potentially irreversible events might include the formation of a world government, the development of AGI, and the first serious efforts at space settlement.

It also gives us a reason to prevent smaller-scale lock-ins—for example, by supporting conservation efforts. Even if we don’t know whether some species or work of art or language is valuable, there is an asymmetry between preserving it and letting it be destroyed. If we preserve it and conclude later that it’s not worth holding on to, then we can always change our minds. If we let it be destroyed, we can’t ever get it back.

### Global Priorities Institute research agenda

The Global Priorities Institute in Oxford researches issues that arise in response to the question, ‘What should we do with a given amount of limited resources if our aim is to do the most good?’. The full research agenda [can be found here](https://globalprioritiesinstitute.org/research-agenda/). Excerpt below.

<hr/>

The idea of the long reflection is that of a long period—perhaps tens of thousands of years—during which human civilisation, perhaps with the aid of improved cognitive ability, dedicates itself to working out what is ultimately of value. It may be argued that such a period would be warranted before deciding whether to undertake an irreversible decision of immense importance, such as whether to attempt spreading to the stars.

### 80,000 Hours podcast interview

This is an interview from January 2018 with William MacAskill, titled [‘Our descendants will probably see us as moral monsters. What should we do about that?’](https://80000hours.org/podcast/episodes/will-macaskill-moral-philosophy/)

You can read the [full transcript here](https://80000hours.org/podcast/episodes/will-macaskill-moral-philosophy/?utm_content=bufferafbcd&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer#transcript).

<hr/>

Robert Wiblin: Before, you mentioned that if humanity doesn’t go extinct in the future, there might be a lot time and a lot of people and very educated people who might be able to do a lot more research on this topic and figure out what’s valuable. That was a long reflection. What do you think that would actually look like in practice, ideally?

Will MacAskill: Yeah. The key idea is just, different people have different sets of values. They might have very different views for what does an optimal future look like. What we really want ideally is a convergent goal between different sorts of values so that we can all say, “Look, this is the thing that we’re all getting behind that we’re trying to ensure that humanity…” Kind of like this is the purpose of civilization. The issue, if you think about purpose of civilization, is just so much disagreement. Maybe there’s something we can aim for that all sorts of different value systems will agree is good. Then, that means we can really get coordination in aiming for that.

I think there is an answer. I call it the long reflection, which is you get to a state where existential risks or extinction risks have been reduced to basically zero. It’s also a position of far greater technological power than we have now, such that we have basically vast intelligence compared to what we have now, amazing empirical understanding of the world, and secondly tens of thousands of years to not really do anything with respect to moving to the stars or really trying to actually build civilization in one particular way, but instead just to engage in this research project of what actually is a value. What actually is the meaning of life? And have, maybe it’s 10 billion people, debating and working on these issues for 10,000 years because the importance is just so great. Humanity, or post-humanity, may be around for billions of years. In which case spending a mere 10,000 is actually absolutely nothing.

In just the same way as if you think as an individual, how much time should you reflect in your own values before choosing your career and committing to one particular path.

Robert Wiblin: Probably at least a few minutes. At least .1% of the whole time.

Will MacAskill: At least a few minutes. Exactly. When you’re thinking about the vastness of the potential future of civilization, the equivalent of just a few minutes is tens of thousands of years.

Then, there’s questions about how exactly do you structure that. I think it would be great if there was more work done really fleshing that out. Perhaps that’s something you’ll have time to do in the near future. One thing you want to do is have as little locked in as possible. So, you want to be very open both on… You don’t want to commit to one particular moral methodology. You just want to commit to things that seem extremely good for basically whatever moral view you might think ends up as correct or what moral epistemology might be correct.

Just people having a higher IQ but everything else being equal, that just seems strictly good. People having greater empirical understanding just seems strictly good. People having a better ability to empathize. That all seems extremely good. People having more time. Have cooperation seems extremely good. Then I think, yeah, like you say, many different people can get behind this one vision for what we want humanity to actually do. That’s potentially exciting because we can coordinate.

It might be that one of the conclusions we come to takes moral uncertainty into account. We might say, actually, there’s some fundamental things that we just can’t ultimately resolve and so we want to do a compromise between them. Maybe that means that for civilization, part of civilization’s devoted to common sense, thick values of pursuit of art, and flourishing, and so on, whereas large parts of the rest of civilization are devoted to other values like pure bliss, blissful state. You can imagine compromise scenarios there. It’s just large amounts of civilization… The universe is a big place.

### Further reading and critical commentary

* [‘Long Reflection’ Is Crazy Bad Idea](https://www.overcomingbias.com/2021/10/long-reflection-is-crazy-bad-idea.html) — Robin Hanson
* [Reflecting on the Long Reflection](https://www.felixstocker.com/blog/reflecting-on-the-long-reflection) — Felix Stocker
* [AI Alignment Podcast: Moral Uncertainty and the Path to AI Alignment with William MacAskill](https://futureoflife.org/podcast/moral-uncertainty-and-the-path-to-ai-alignment-with-william-macaskill/) — Future of Life Podcast
* [Cause prioritization for downside-focused value systems](https://longtermrisk.org/cause-prioritization-downside-focused-value-systems/#Moral_uncertainty_and_cooperation) — Lukas Gloor
* [Quotes about the long reflection](https://forum.effectivealtruism.org/posts/H2zno3ggRJaph9P6c/quotes-about-the-long-reflection) — Michael Aird

[‘Long Reflection’ tag](https://forum.effectivealtruism.org/topics/long-reflection) on the [Effective Altruism Forum](https://forum.effectivealtruism.org/)